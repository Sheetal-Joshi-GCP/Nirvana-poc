{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_jyqAW_2Ouu",
    "outputId": "ad658552-3862-4cd8-c246-19532186d008"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "! pip install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q\n",
    "! pip install kfp==1.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr4jnWxh2Oux"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_versions:kfp",
    "outputId": "7b45c8f0-cdc7-41ca-8bfc-d68b80554d0d"
   },
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"vertex-nirvana-poc\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "04933ed28eef"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jvNx3KyF2Ou0"
   },
   "outputs": [],
   "source": [
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lFCteCD32Ou0"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"child-pipelines\"  # Replace with your bucket name\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "source": [
    "#### Vertex AI constants\n",
    "\n",
    "Setup up the following constants for Vertex AI:\n",
    "\n",
    "- `API_ENDPOINT`: The Vertex AI API service endpoint for `Dataset`, `Model`, `Job`, `Pipeline` and `Endpoint` services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TnO8gVBb2Ou4"
   },
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI Pipelines constants\n",
    "\n",
    "Setup up the following constants for Vertex AI Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ONHtQDz32Ou5"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbpw7oyM2Ou5"
   },
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI SDK for your project and bucket\n",
    "# TODO 1: Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GjJhJUID2Ou6"
   },
   "outputs": [],
   "source": [
    "@component(output_component_file=\"C1.yaml\", base_image=\"python:3.9\")\n",
    "def Input_Number(number: str) -> str:\n",
    "    return number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWcIXuxR2Ou6"
   },
   "source": [
    "As you'll see below, compilation of this component creates a [task factory function](https://www.kubeflow.or/docs/components/pipelines/sdk/python-function-components/)—called `hello_world`— that you can use in defining a pipeline step.\n",
    "\n",
    "While not shown here, if you want to share this componentdefinition, or use it in another context, you could also load it from its yaml file like this:\n",
    "`hello_world_op = components.load_component_from_file('./hw.yaml')`.\n",
    "You can also use the `load_component_from_url` method, if your component yaml file is stored online. (For GitHub URLs, load the 'raw' file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:two_outputs"
   },
   "source": [
    "#### Define two_outputs component\n",
    "\n",
    "The first component below, `two_outputs`, demonstrates installing a package -- in this case the `google-cloud-storage` package. Alternatively, you can specify a base image that includes the necessary installations.\n",
    "\n",
    "*Note:* The component function won't actually use the package.\n",
    "\n",
    "Alternatively, you can specify a base image that includes the necessary installations.\n",
    "\n",
    "The `two_outputs` component returns two named outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "U4Yv33su2Ou6"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-storage\"])\n",
    "def Add_Two_Numbers(\n",
    "    number: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_one\", str),  # Return parameters\n",
    "        (\"output_two\", str),\n",
    "    ],\n",
    "):\n",
    "    # the import is not actually used for this simple example, but the import\n",
    "    # is successful, as it was included in the `packages_to_install` list.\n",
    "    from google.cloud import storage  \n",
    "\n",
    "    o1 = f\"output one from number + 7: {int(number)+7}\"\n",
    "    o2 = f\"output two from number + 5: {int(number)+5}\"\n",
    "    print(\"output one: {}; output_two: {}\".format(o1, o2))\n",
    "    return (o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bu8XvOj82Ou6"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "def sub_pipeline(number1: str, number2: str, number3: str):\n",
    "    print(f\"number1: {number1}; number2: {number2}; number3: {number3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qeVTW_3j6bxd"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "def trigger_childflow(message : str) -> str:\n",
    "    print(f\"The next flow message is {message}\")\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:intro"
   },
   "source": [
    "### Define a pipeline that uses the components\n",
    "\n",
    "Next, define a pipeline that uses these three components.\n",
    "\n",
    "By evaluating the component definitions above, you've created task factory functions that are used in the pipeline definition to create the pipeline steps.\n",
    "\n",
    "The pipeline takes an input parameter, and passes that parameter as an argument to the first two pipeline steps (`hw_task` and `two_outputs_task`).\n",
    "\n",
    "Then, the third pipeline step (`consumer_task`) consumes the outputs of the first and second steps.  Because the `hello_world` component definition just returns one unnamed output, you refer to it as `hw_task.output`.  The `two_outputs` task returns two named outputs, which you access as `two_outputs_task.outputs[\"<output_name>\"]`.\n",
    "\n",
    "*Note:* In the `@dsl.pipeline` decorator, you're defining the `PIPELINE_ROOT` Cloud Storage path to use.  If you had not included that info here, it would be required to specify it when creating the pipeline run, as you'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "V8qjpwGK5A8L"
   },
   "outputs": [],
   "source": [
    "# Define the first pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"pipeline1\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline1(number: str = \"999\"):\n",
    "    hw_task = Input_Number(number)\n",
    "    two_outputs_task = Add_Two_Numbers(hw_task.output)\n",
    "    # Call the second pipeline\n",
    "    pipeline2_task = pipeline2(two_outputs_task.outputs[\"output_one\"], two_outputs_task.outputs[\"output_two\"])\n",
    "\n",
    "\n",
    "# Define the second pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"pipeline2\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline2(number1_output: str, number2_output: str):\n",
    "    # Use the two_outputs component\n",
    "    next_flow = trigger_childflow(\"The addition of two numbers are done on pipeline 1\")\n",
    "    consumer_task = sub_pipeline(  \n",
    "        number1=next_flow.output,\n",
    "        number2=number1_output,\n",
    "        number3=number2_output,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VP_JJ9Oe2Ou7"
   },
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "from kfp.v2 import compiler  \n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline1, package_path=\"two-pipelines-v2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:intro"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Next, run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjxaBix_2Ou7",
    "outputId": "ce05ac85-c0a7-47d8-86fc-e2f7244d2219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/665554893092/locations/us-central1/pipelineJobs/pipeline1-20231030113026\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/665554893092/locations/us-central1/pipelineJobs/pipeline1-20231030113026')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline1-20231030113026?project=665554893092\n"
     ]
    }
   ],
   "source": [
    "# Create and run the pipeline job run\n",
    "DISPLAY_NAME = \"intro_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"two-pipelines-v2.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d428cc8803e7"
   },
   "source": [
    "### Delete the pipeline job\n",
    "\n",
    "You can delete the pipeline job with the method `delete()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97802d9432e7",
    "outputId": "106e80f4-23c8-48dd-aeb5-c80140547bd2"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
